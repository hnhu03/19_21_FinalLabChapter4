{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          add    ai_ngờ        ak  all      alot      also    always  \\\n",
      "0    0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n",
      "1    0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n",
      "2    0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n",
      "3    0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n",
      "4    0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n",
      "..        ...       ...       ...  ...       ...       ...       ...   \n",
      "688  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n",
      "689  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n",
      "690  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n",
      "691  0.000000  0.000000  0.000000  0.0  0.000000  0.000000  0.000000   \n",
      "692  0.004272  0.004272  0.004272  0.0  0.004272  0.004272  0.004272   \n",
      "\n",
      "     ambiance   an  an_duyên  ...        ốc   ốc_bươu        ốm        ốp  \\\n",
      "0    0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "1    0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "2    0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "3    0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "4    0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "..        ...  ...       ...  ...       ...       ...       ...       ...   \n",
      "688  0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "689  0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "690  0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "691  0.000000  0.0  0.000000  ...  0.000000  0.000000  0.000000  0.000000   \n",
      "692  0.004272  0.0  0.004272  ...  0.022262  0.004272  0.004272  0.004272   \n",
      "\n",
      "           ồm        ồn       ổn        ớt        ức  ức_chế  \n",
      "0    0.000000  0.000000  0.00000  0.000000  0.000000  0.0000  \n",
      "1    0.000000  0.000000  0.00000  0.000000  0.000000  0.0000  \n",
      "2    0.000000  0.000000  0.00000  0.000000  0.000000  0.0000  \n",
      "3    0.000000  0.000000  0.00000  0.000000  0.000000  0.0000  \n",
      "4    0.000000  0.000000  0.00000  0.000000  0.000000  0.0000  \n",
      "..        ...       ...      ...       ...       ...     ...  \n",
      "688  0.000000  0.000000  0.00000  0.000000  0.000000  0.0000  \n",
      "689  0.000000  0.000000  0.00000  0.000000  0.000000  0.0000  \n",
      "690  0.000000  0.000000  0.00000  0.000000  0.000000  0.0000  \n",
      "691  0.000000  0.000000  0.00000  0.000000  0.000000  0.0000  \n",
      "692  0.004272  0.023601  0.03353  0.008544  0.004272  0.0118  \n",
      "\n",
      "[693 rows x 2174 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "import emoji\n",
    "\n",
    "# Khởi tạo đối tượng CountVectorizer\n",
    "# Tham số ngram_range=(1,2) cho biết chúng ta muốn sử dụng N-grams từ 1 đến 2.\n",
    "vectorizer = CountVectorizer(ngram_range=(1,1))\n",
    "v=[]\n",
    "textt=''\n",
    "f = open(\"D:/vnm/vnm/vietnamese-stopwords.txt\", \"r\", encoding=\"utf-8\")\n",
    "#Get Stop words Dictionaries\n",
    "List_StopWords=f.read().split(\"\\n\")\n",
    "#Read file text\n",
    "src='D:/KhAI PHA WEB/19/Data1-20231102T012633Z-001/Data1'\n",
    "path=os.listdir(src)\n",
    "for i in path:\n",
    "    label=i.split(\"'\")[0]\n",
    "    f=open('D:/KhAI PHA WEB/19/Data1-20231102T012633Z-001/Data1/'+str(label), \"r\", encoding=\"utf-8\")\n",
    "    text=f.read()\n",
    "    text_pre=text.replace(\"\\n\",\"\")  # Remove the newline command\n",
    "    text_pre=text.lower() # Convert text to lowercase\n",
    "    text_pre = re.sub(\"\\d+\", \" \", text_pre) # Remove number\n",
    "    text_pre = re.sub(r\"[!@#$[]()]'\", \"\", text_pre) # Remove character: !@#$[]()\n",
    "    text_pre=emoji.demojize(text_pre)\n",
    "    stop = stopwords.words('english')   # Remove StopWords\n",
    "    text_pre = \" \".join(text_pre for text_pre in text_pre.split() if text_pre not in List_StopWords)\n",
    "    textt=textt +text_pre \n",
    "v=textt.split(\" . \")\n",
    "Data=[]\n",
    "for i in v:\n",
    "    text_i=re.sub(r'[^\\w\\s]','',i) # Remove punctuation\n",
    "    Data.append(text_i)\n",
    "Data=list(filter(None,Data))\n",
    "tr_idf_model  = TfidfVectorizer()\n",
    "tf_idf_vector = tr_idf_model.fit_transform(Data)\n",
    "tf_idf_array = tf_idf_vector.toarray()\n",
    "W = tr_idf_model.get_feature_names_out()\n",
    "df_tf_idf = pd.DataFrame(tf_idf_array, columns = W)\n",
    "print(df_tf_idf)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
